# Problem 4.24 實驗報告

## 一、實驗設計

### 1.1 目標
研究留一法交叉驗證(LOOCV)在線性迴歸中的統計特性，分析交叉驗證誤差的期望值、變異數及有效樣本數。

### 1.2 參數設定
- **維度**: d = 3
- **樣本數**: N ∈ {18, 28, 38, ..., 118} (共11個值)
- **噪音標準差**: σ = 0.5
- **正規化參數**: λ = 0.05/N 和 λ = 2.5/N
- **實驗次數**: 100,000 次

### 1.3 數據生成
- 輸入 X: 每維度從標準常態分佈 N(0,1) 採樣
- 目標權重 w_f: 從 N(0,1) 採樣
- 目標值: y = w_f^T x + 0.5ε，其中 ε ~ N(0,1)

### 1.4 方法
使用嶺迴歸 (Ridge Regression) 進行訓練：
- w_reg = (X^T X + λI)^(-1) X^T y
- 對每個數據點執行留一法交叉驗證
- 記錄 e_1, e_2, E_cv 及其統計量

---

## 二、實驗結果數據

### 2.1 主要統計量 (λ = 0.05/N)

| N   | E[e₁]    | E[E_cv]  | Var[e₁]  | Var[E_cv] | N_eff  | N_eff/N |
|-----|----------|----------|----------|-----------|--------|---------|
| 18  | 0.331    | 0.331    | 0.232    | 0.017     | 13.67  | 75.96%  |
| 38  | 0.280    | 0.281    | 0.158    | 0.005     | 33.33  | 87.71%  |
| 68  | 0.266    | 0.266    | 0.142    | 0.002     | 63.60  | 93.53%  |
| 98  | 0.261    | 0.261    | 0.134    | 0.001     | 92.50  | 94.39%  |
| 118 | 0.260    | 0.259    | 0.136    | 0.001     | 115.22 | 97.64%  |

**關鍵發現**:
- E[e₁] ≈ E[E_cv] (差異 < 0.001)
- Var[E_cv] ≈ Var[e₁]/N
- N_eff/N > 97% (當 N=118)

### 2.2 正規化參數影響 (N=118)

| λ 設定    | N_eff/N | 變化   |
|-----------|---------|--------|
| 0.05/N    | 97.64%  | -      |
| 2.5/N     | 97.78%  | +0.14% |

**結論**: 正規化參數對 N_eff 影響極小

---

## 三、圖表說明

### 圖1: Part (b) - E[e₁], E[e₂], E[E_cv] 關係圖
**檔案**: `Pr4.24_part_b.png`

**內容**: 三條曲線顯示不同交叉驗證誤差的期望值隨 N 的變化

**說明**:
- 三條曲線幾乎完全重合 → 驗證理論: E[e₁] = E[e₂] = E[E_cv]
- 所有誤差隨 N 增加而遞減 → 更多數據帶來更好的泛化能力
- 從 0.331 (N=18) 降至 0.259 (N=118)，減少約 22%

**學習現象**: 留一法交叉驗證提供無偏的泛化誤差估計

---

### 圖2: Part (c) - Var[e₁] vs Var[E_cv]
**檔案**: `Pr4.24_part_c.png`

**內容**: 比較單個 CV 誤差與平均 CV 誤差的變異數

**說明**:
- 藍線 (Var[e₁]): 單點誤差變異數，緩慢下降
- 紅線 (Var[E_cv]): 平均誤差變異數，急劇下降
- 兩者差距隨 N 增加而擴大，比值接近 N

**學習現象**: 平均 N 個誤差顯著降低變異數，使估計更穩定可靠

---

### 圖3: Part (e) - 有效樣本數分析
**檔案**: `Pr4.24_part_e.png`

**內容**: 
- 左圖: N_eff 絕對值 vs N
- 右圖: N_eff/N 百分比

**說明**:
- 綠線幾乎沿著紅色參考線 (y=N) → N_eff ≈ N
- N_eff/N 從 76% 上升到 98%
- 大樣本時接近 100%

**學習現象**: 
- LOOCV 的 CV 誤差幾乎完全獨立
- 充分利用數據，有效樣本數接近實際樣本數
- **關鍵機制**: 每個誤差使用不同的驗證點

---

### 圖4: Part (f) - 正規化參數比較
**檔案**: `Pr4.24_part_f_comparison.png`

**內容**: 比較 λ=0.05/N 和 λ=2.5/N 的 N_eff

**說明**:
- 左圖: 兩種 λ 的 N_eff 曲線幾乎重合
- 右圖: N_eff/N 百分比差異極小 (< 2%)
- 藍線 (λ=0.05/N) 和橙線 (λ=2.5/N) 交織

**學習現象**:
- 增加 50 倍正規化參數，N_eff 變化微小
- 驗證點獨立性主導效應，超過訓練集相似性的影響
- 輕度到中度正規化不會顯著改變 CV 行為

---

## 四、結論

### 4.1 核心發現

1. **無偏性** (Part b): E[CV誤差] = E[泛化誤差]，實驗誤差 < 0.3%

2. **變異數降低** (Part c): 平均效應使 Var[E_cv] ≈ Var[e_i]/N

3. **高效數據利用** (Part e): N_eff/N > 97%，LOOCV 充分利用所有數據

4. **正規化影響微弱** (Part f): λ 增加 50 倍，N_eff 僅變化 0.14%

### 4.2 實務建議

**何時使用 LOOCV**:
- ✅ 小數據集 (N < 1000)
- ✅ 需要精確估計
- ✅ 線性模型或快速訓練的模型

**注意事項**:
- ❌ 大數據集計算成本高 → 使用 k-fold CV
- ❌ 複雜模型訓練慢 → 使用 5-fold CV

### 4.3 統計意義

LOOCV 提供:
- 最大化的數據利用率 (>97%)
- 無偏的泛化誤差估計
- 低變異數的性能評估

這些特性使 LOOCV 成為小數據機器學習的**最佳方法**。

---

**實驗完成日期**: 2025年10月7日  
**計算資源**: 100,000 次實驗 × 11 個 N 值 × 2 個 λ 設定 = 2,200,000 次模型訓練  
**運行時間**: 約 25 分鐘
