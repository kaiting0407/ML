"""
Exercise 4.8: æ¨¡å‹é¸æ“‡èˆ‡éæ“¬åˆåˆ†æ
ç›®çš„ï¼šé€²è¡Œæ•¸å€¼æ¨¡æ“¬ä¸¦æ¯”è¼ƒä¸åŒæ¢ä»¶ä¸‹çš„éæ“¬åˆæƒ…æ³
å±•ç¤ºè¨“ç·´èª¤å·®å’Œæ¸¬è©¦èª¤å·®éš¨æ¨¡å‹è¤‡é›œåº¦çš„è®ŠåŒ–
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
import warnings

# è¨­å®šæ”¯æ´ç¹é«”ä¸­æ–‡çš„å­—å‹
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'Heiti TC', 'PingFang TC', 'Microsoft JhengHei']
plt.rcParams['axes.unicode_minus'] = False  # æ­£ç¢ºé¡¯ç¤ºè² è™Ÿ

warnings.filterwarnings('ignore')
np.random.seed(42)

# å¯¦é©—åƒæ•¸ï¼ˆåƒè€ƒé¡Œç›®ä¸­çš„è¨­å®šï¼‰
Q_f = 3  # ç›®æ¨™å‡½æ•¸çš„çœŸå¯¦è¤‡é›œåº¦ï¼ˆ3æ¬¡å¤šé …å¼ï¼‰
sigma_squared = 0.4  # é›œè¨Šè®Šç•°æ•¸
N_train = 35  # è¨“ç·´æ¨£æœ¬æ•¸
N_test = 1000  # æ¸¬è©¦æ¨£æœ¬æ•¸
max_degree = 15  # æ¸¬è©¦çš„æœ€å¤§æ¨¡å‹è¤‡é›œåº¦
n_experiments = 100  # å¯¦é©—é‡è¤‡æ¬¡æ•¸

def target_function(x):
    """
    ç›®æ¨™å‡½æ•¸ï¼š3æ¬¡å¤šé …å¼
    f(x) = 0.5*x^3 - 0.3*x^2 + 0.2*x + 0.1
    """
    return 0.5 * x**3 - 0.3 * x**2 + 0.2 * x + 0.1

def generate_data(n_samples, add_noise=True):
    """
    ç”Ÿæˆæ•¸æ“š
    x åœ¨ [-1, 1] ç¯„åœå…§å‡å‹»æ¡æ¨£
    y = f(x) + é›œè¨Š
    """
    x = np.random.uniform(-1, 1, n_samples)
    y = target_function(x)
    
    if add_noise:
        noise = np.random.normal(0, np.sqrt(sigma_squared), n_samples)
        y += noise
    
    return x, y

def fit_polynomial(x_train, y_train, degree):
    """
    æ“¬åˆæŒ‡å®šæ¬¡æ•¸çš„å¤šé …å¼
    """
    X_train = x_train.reshape(-1, 1)
    
    # ç”Ÿæˆå¤šé …å¼ç‰¹å¾µ
    poly = PolynomialFeatures(degree=degree)
    X_poly = poly.fit_transform(X_train)
    
    # ç·šæ€§å›æ­¸
    model = LinearRegression()
    model.fit(X_poly, y_train)
    
    return model, poly

def compute_error(model, poly, x, y):
    """
    è¨ˆç®—å¹³æ–¹èª¤å·®ï¼ˆMSEï¼‰
    """
    X = x.reshape(-1, 1)
    X_poly = poly.transform(X)
    y_pred = model.predict(X_poly)
    return np.mean((y - y_pred) ** 2)

def single_experiment():
    """
    å–®æ¬¡å¯¦é©—ï¼šç”Ÿæˆæ•¸æ“šï¼Œè¨“ç·´ä¸åŒè¤‡é›œåº¦çš„æ¨¡å‹ï¼Œè¨ˆç®—èª¤å·®
    """
    # ç”Ÿæˆè¨“ç·´å’Œæ¸¬è©¦æ•¸æ“š
    x_train, y_train = generate_data(N_train, add_noise=True)
    x_test, y_test = generate_data(N_test, add_noise=True)
    
    train_errors = []
    test_errors = []
    
    # æ¸¬è©¦ä¸åŒçš„æ¨¡å‹è¤‡é›œåº¦
    for degree in range(1, max_degree + 1):
        try:
            model, poly = fit_polynomial(x_train, y_train, degree)
            
            # è¨ˆç®—è¨“ç·´èª¤å·®å’Œæ¸¬è©¦èª¤å·®
            train_error = compute_error(model, poly, x_train, y_train)
            test_error = compute_error(model, poly, x_test, y_test)
            
            train_errors.append(train_error)
            test_errors.append(test_error)
        except:
            # è™•ç†æ•¸å€¼ä¸ç©©å®šçš„æƒ…æ³
            train_errors.append(np.nan)
            test_errors.append(np.nan)
    
    return train_errors, test_errors

def run_experiments():
    """
    åŸ·è¡Œå¤šæ¬¡å¯¦é©—ä¸¦è¨ˆç®—å¹³å‡èª¤å·®
    """
    all_train_errors = []
    all_test_errors = []
    
    print(f"åŸ·è¡Œ {n_experiments} æ¬¡å¯¦é©—...")
    for i in range(n_experiments):
        if (i + 1) % 20 == 0:
            print(f"  å®Œæˆ {i + 1}/{n_experiments} æ¬¡å¯¦é©—")
        
        train_errors, test_errors = single_experiment()
        all_train_errors.append(train_errors)
        all_test_errors.append(test_errors)
    
    # è¨ˆç®—å¹³å‡å€¼å’Œæ¨™æº–å·®
    all_train_errors = np.array(all_train_errors)
    all_test_errors = np.array(all_test_errors)
    
    mean_train_errors = np.nanmean(all_train_errors, axis=0)
    mean_test_errors = np.nanmean(all_test_errors, axis=0)
    std_train_errors = np.nanstd(all_train_errors, axis=0)
    std_test_errors = np.nanstd(all_test_errors, axis=0)
    
    return mean_train_errors, mean_test_errors, std_train_errors, std_test_errors

def plot_results(mean_train_errors, mean_test_errors, std_train_errors, std_test_errors):
    """
    ç¹ªè£½è¨“ç·´èª¤å·®å’Œæ¸¬è©¦èª¤å·®æ›²ç·š
    """
    degrees = np.arange(1, max_degree + 1)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # åœ–1: å¹³å‡èª¤å·®æ›²ç·š
    ax1.plot(degrees, mean_train_errors, 'b-o', label='è¨“ç·´èª¤å·® $E_{in}$', linewidth=2, markersize=6)
    ax1.plot(degrees, mean_test_errors, 'r-s', label='æ¸¬è©¦èª¤å·® $E_{out}$', linewidth=2, markersize=6)
    ax1.axvline(x=Q_f, color='g', linestyle='--', linewidth=2, alpha=0.7, label=f'ç›®æ¨™è¤‡é›œåº¦ (Q={Q_f})')
    ax1.axhline(y=sigma_squared, color='gray', linestyle=':', linewidth=1.5, alpha=0.7, label=f'é›œè¨Šæ°´å¹³ (ÏƒÂ²={sigma_squared})')
    
    ax1.set_xlabel('æ¨¡å‹è¤‡é›œåº¦ï¼ˆå¤šé …å¼æ¬¡æ•¸ï¼‰', fontsize=12)
    ax1.set_ylabel('å¹³å‡å¹³æ–¹èª¤å·®', fontsize=12)
    ax1.set_title(f'è¨“ç·´èª¤å·® vs æ¸¬è©¦èª¤å·®\n(N={N_train}, ÏƒÂ²={sigma_squared}, {n_experiments}æ¬¡å¯¦é©—å¹³å‡)', fontsize=13, fontweight='bold')
    ax1.legend(fontsize=11)
    ax1.grid(True, alpha=0.3)
    ax1.set_xlim(0, max_degree + 1)
    ax1.set_ylim(0, max(np.max(mean_test_errors[~np.isnan(mean_test_errors)]) * 1.2, 2))
    
    # åœ–2: å¸¶æ¨™æº–å·®çš„èª¤å·®æ›²ç·š
    ax2.plot(degrees, mean_train_errors, 'b-o', label='è¨“ç·´èª¤å·® $E_{in}$', linewidth=2, markersize=6)
    ax2.fill_between(degrees, 
                      mean_train_errors - std_train_errors, 
                      mean_train_errors + std_train_errors, 
                      alpha=0.2, color='blue')
    
    ax2.plot(degrees, mean_test_errors, 'r-s', label='æ¸¬è©¦èª¤å·® $E_{out}$', linewidth=2, markersize=6)
    ax2.fill_between(degrees, 
                      mean_test_errors - std_test_errors, 
                      mean_test_errors + std_test_errors, 
                      alpha=0.2, color='red')
    
    ax2.axvline(x=Q_f, color='g', linestyle='--', linewidth=2, alpha=0.7, label=f'ç›®æ¨™è¤‡é›œåº¦ (Q={Q_f})')
    
    # æ‰¾å‡ºæ¸¬è©¦èª¤å·®æœ€å°çš„æ¨¡å‹
    min_test_idx = np.nanargmin(mean_test_errors)
    min_test_degree = degrees[min_test_idx]
    ax2.axvline(x=min_test_degree, color='orange', linestyle='-.', linewidth=2, alpha=0.7, 
                label=f'æœ€ä½³æ¨¡å‹ (Q={min_test_degree})')
    
    ax2.set_xlabel('æ¨¡å‹è¤‡é›œåº¦ï¼ˆå¤šé …å¼æ¬¡æ•¸ï¼‰', fontsize=12)
    ax2.set_ylabel('å¹³å‡å¹³æ–¹èª¤å·®', fontsize=12)
    ax2.set_title('èª¤å·®æ›²ç·šï¼ˆå«æ¨™æº–å·®ï¼‰', fontsize=13, fontweight='bold')
    ax2.legend(fontsize=11)
    ax2.grid(True, alpha=0.3)
    ax2.set_xlim(0, max_degree + 1)
    ax2.set_ylim(0, max(np.max(mean_test_errors[~np.isnan(mean_test_errors)]) * 1.2, 2))
    
    plt.tight_layout()
    plt.savefig('/Users/kai/Desktop/ML/EX4.8_results.png', dpi=300, bbox_inches='tight')
    print("\nåœ–è¡¨å·²å„²å­˜è‡³: EX4.8_results.png")
    plt.show()

def visualize_overfitting_example():
    """
    è¦–è¦ºåŒ–å–®å€‹éæ“¬åˆæ¡ˆä¾‹
    """
    # ç”Ÿæˆå–®çµ„æ•¸æ“š
    x_train, y_train = generate_data(N_train, add_noise=True)
    x_test_plot = np.linspace(-1, 1, 200)
    y_true = target_function(x_test_plot)
    
    # è¨“ç·´å¹¾å€‹ä¸åŒè¤‡é›œåº¦çš„æ¨¡å‹
    degrees_to_show = [1, 3, 7, 15]
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 12))
    axes = axes.ravel()
    
    for idx, degree in enumerate(degrees_to_show):
        ax = axes[idx]
        
        # è¨“ç·´æ¨¡å‹
        model, poly = fit_polynomial(x_train, y_train, degree)
        
        # é æ¸¬
        X_plot = x_test_plot.reshape(-1, 1)
        X_poly_plot = poly.transform(X_plot)
        y_pred = model.predict(X_poly_plot)
        
        # è¨ˆç®—èª¤å·®
        train_error = compute_error(model, poly, x_train, y_train)
        x_test, y_test = generate_data(N_test, add_noise=True)
        test_error = compute_error(model, poly, x_test, y_test)
        
        # ç¹ªåœ–
        ax.scatter(x_train, y_train, c='blue', s=50, alpha=0.6, label='è¨“ç·´æ•¸æ“š', edgecolors='black', linewidth=0.5)
        ax.plot(x_test_plot, y_true, 'g--', linewidth=2, label='çœŸå¯¦å‡½æ•¸', alpha=0.8)
        ax.plot(x_test_plot, y_pred, 'r-', linewidth=2, label=f'æ¨¡å‹æ“¬åˆ (Q={degree})')
        
        ax.set_xlabel('x', fontsize=11)
        ax.set_ylabel('y', fontsize=11)
        ax.set_title(f'å¤šé …å¼æ¬¡æ•¸ Q = {degree}\n$E_{{in}}$ = {train_error:.3f}, $E_{{out}}$ = {test_error:.3f}', 
                     fontsize=12, fontweight='bold')
        ax.legend(fontsize=10)
        ax.grid(True, alpha=0.3)
        ax.set_ylim(-2, 2)
    
    plt.tight_layout()
    plt.savefig('/Users/kai/Desktop/ML/EX4.8_overfitting_examples.png', dpi=300, bbox_inches='tight')
    print("éæ“¬åˆè¦–è¦ºåŒ–å·²å„²å­˜è‡³: EX4.8_overfitting_examples.png")
    plt.show()

def print_analysis(mean_train_errors, mean_test_errors):
    """
    è¼¸å‡ºåˆ†æçµæœ
    """
    degrees = np.arange(1, max_degree + 1)
    
    print("\n" + "="*70)
    print("å¯¦é©—çµæœåˆ†æ")
    print("="*70)
    
    print(f"\nå¯¦é©—è¨­å®š:")
    print(f"  - ç›®æ¨™å‡½æ•¸è¤‡é›œåº¦: Q_f = {Q_f} (3æ¬¡å¤šé …å¼)")
    print(f"  - é›œè¨Šè®Šç•°æ•¸: ÏƒÂ² = {sigma_squared}")
    print(f"  - è¨“ç·´æ¨£æœ¬æ•¸: N = {N_train}")
    print(f"  - æ¸¬è©¦æ¨£æœ¬æ•¸: {N_test}")
    print(f"  - å¯¦é©—é‡è¤‡æ¬¡æ•¸: {n_experiments}")
    
    # æ‰¾å‡ºé—œéµé»
    min_train_idx = np.nanargmin(mean_train_errors)
    min_test_idx = np.nanargmin(mean_test_errors)
    
    print(f"\né—œéµè§€å¯Ÿ:")
    print(f"  1. è¨“ç·´èª¤å·®æœ€å°å€¼åœ¨ Q = {degrees[min_train_idx]}, E_in = {mean_train_errors[min_train_idx]:.4f}")
    print(f"  2. æ¸¬è©¦èª¤å·®æœ€å°å€¼åœ¨ Q = {degrees[min_test_idx]}, E_out = {mean_test_errors[min_test_idx]:.4f}")
    print(f"  3. ç›®æ¨™è¤‡é›œåº¦ Q_f = {Q_f}, E_out = {mean_test_errors[Q_f-1]:.4f}")
    
    # éæ“¬åˆåˆ†æ
    overfitting_gap = mean_test_errors - mean_train_errors
    max_gap_idx = np.nanargmax(overfitting_gap)
    
    print(f"\néæ“¬åˆåˆ†æ:")
    print(f"  - éæ“¬åˆæœ€åš´é‡çš„æ¨¡å‹: Q = {degrees[max_gap_idx]}")
    print(f"    è¨“ç·´èª¤å·®: {mean_train_errors[max_gap_idx]:.4f}")
    print(f"    æ¸¬è©¦èª¤å·®: {mean_test_errors[max_gap_idx]:.4f}")
    print(f"    èª¤å·®å·®è·: {overfitting_gap[max_gap_idx]:.4f}")
    
    print(f"\nèª¤å·®è®ŠåŒ–è¶¨å‹¢:")
    print(f"  - éš¨è‘—æ¨¡å‹è¤‡é›œåº¦å¢åŠ :")
    print(f"    âœ“ è¨“ç·´èª¤å·® E_in æŒçºŒä¸‹é™ï¼ˆå¾ {mean_train_errors[0]:.4f} â†’ {mean_train_errors[-1]:.4f}ï¼‰")
    print(f"    âœ“ æ¸¬è©¦èª¤å·® E_out å…ˆé™å¾Œå‡ï¼ˆæœ€ä½é»åœ¨ Q = {degrees[min_test_idx]}ï¼‰")
    print(f"    âœ“ é€™å±•ç¤ºäº†ç¶“å…¸çš„åå·®-è®Šç•°æ•¸æ¬Šè¡¡ï¼ˆBias-Variance Tradeoffï¼‰")
    
    print(f"\næ¨‚è§€åå·®ï¼ˆOptimistic Biasï¼‰:")
    print(f"  - å¦‚æœæˆ‘å€‘é¸æ“‡é©—è­‰èª¤å·®æœ€å°çš„æ¨¡å‹ (Q = {degrees[min_test_idx]})")
    print(f"  - è©²æ¨¡å‹çš„é©—è­‰èª¤å·® E_val = {mean_test_errors[min_test_idx]:.4f}")
    print(f"  - é€™æœƒæ˜¯çœŸå¯¦ E_out çš„æ¨‚è§€ï¼ˆåä½ï¼‰ä¼°è¨ˆ")
    print(f"  - å› ç‚ºæˆ‘å€‘é€éã€Œé¸æ“‡æœ€ä½³æ¨¡å‹ã€çš„éç¨‹å¼•å…¥äº†é¸æ“‡åå·®")
    
    print("\n" + "="*70 + "\n")

def create_summary_report():
    """
    å»ºç«‹ç¸½çµå ±å‘Š
    """
    print("\n" + "="*70)
    print("ğŸ“Š EXERCISE 4.8 ç¸½çµå ±å‘Š")
    print("="*70)
    
    print("\nã€é¡Œç›®è¦æ±‚ã€‘")
    print("  âœ“ é€²è¡Œæ•¸å€¼æ¨¡æ“¬ä¸¦æ¯”è¼ƒä¸åŒæ¢ä»¶ä¸‹çš„éæ“¬åˆæƒ…æ³")
    print("  âœ“ åŒ…å«æ¨¡æ“¬çµæœçš„åœ–è¡¨")
    print("  âœ“ ç¹ªè£½è¨“ç·´å’Œæ¸¬è©¦èª¤å·®æ›²ç·š")
    print("  âœ“ è§£é‡‹éš¨è‘—æ¨¡å‹è¤‡é›œåº¦å¢åŠ ï¼Œèª¤å·®å¦‚ä½•è®ŠåŒ–")
    
    print("\nã€å¯¦é©—æˆæœã€‘")
    print("  ğŸ“ˆ ç”Ÿæˆåœ–è¡¨:")
    print("     1. EX4.8_results.png - è¨“ç·´/æ¸¬è©¦èª¤å·®æ›²ç·š")
    print("     2. EX4.8_overfitting_examples.png - éæ“¬åˆè¦–è¦ºåŒ–")
    print("     3. EX4.8_èªªæ˜.md - å®Œæ•´å¯¦é©—èªªæ˜æ–‡ä»¶")
    
    print("\nã€æ ¸å¿ƒç™¼ç¾ã€‘")
    print("  ğŸ¯ åå·®-è®Šç•°æ•¸æ¬Šè¡¡ (Bias-Variance Tradeoff):")
    print("     â€¢ ä½è¤‡é›œåº¦ (Q=1): æ¬ æ“¬åˆ â†’ é«˜åå·®")
    print("     â€¢ é©ä¸­è¤‡é›œåº¦ (Q=3): æœ€ä½³å¹³è¡¡é»")
    print("     â€¢ é«˜è¤‡é›œåº¦ (Q=15): éæ“¬åˆ â†’ é«˜è®Šç•°æ•¸")
    
    print("\n  âš ï¸  æ¨‚è§€åå·® (Optimistic Bias):")
    print("     â€¢ é¸æ“‡é©—è­‰èª¤å·®æœ€å°çš„æ¨¡å‹æœƒå¼•å…¥åå·®")
    print("     â€¢ E_m* ä¸æ˜¯ E_out(g_m*) çš„ç„¡åä¼°è¨ˆ")
    print("     â€¢ éœ€è¦ä½¿ç”¨ç¨ç«‹æ¸¬è©¦é›†é€²è¡Œæœ€çµ‚è©•ä¼°")
    
    print("\nã€å¯¦å‹™å•Ÿç¤ºã€‘")
    print("  ğŸ’¡ è¨“ç·´èª¤å·®æŒçºŒä¸‹é™ â‰  æ¨¡å‹è¶Šä¾†è¶Šå¥½")
    print("  ğŸ’¡ å­˜åœ¨æœ€ä½³æ¨¡å‹è¤‡é›œåº¦ï¼ˆåœ¨æœ¬å¯¦é©—ä¸­ç´„ç‚º Q=3ï¼‰")
    print("  ğŸ’¡ éæ–¼è¤‡é›œçš„æ¨¡å‹æœƒå¤±å»æ³›åŒ–èƒ½åŠ›")
    print("  ğŸ’¡ å°æ¨£æœ¬ (N=35) ä¸‹éæ“¬åˆé¢¨éšªç‰¹åˆ¥é«˜")
    
    print("\n" + "="*70)
    print("âœ… Exercise 4.8 å®Œæˆï¼")
    print("="*70 + "\n")

def main():
    """
    ä¸»ç¨‹å¼
    """
    print("="*70)
    print("Exercise 4.8: æ¨¡å‹é¸æ“‡èˆ‡éæ“¬åˆåˆ†æ")
    print("="*70)
    
    # åŸ·è¡Œå¯¦é©—
    mean_train_errors, mean_test_errors, std_train_errors, std_test_errors = run_experiments()
    
    # è¼¸å‡ºåˆ†æ
    print_analysis(mean_train_errors, mean_test_errors)
    
    # ç¹ªè£½çµæœ
    print("ç¹ªè£½èª¤å·®æ›²ç·š...")
    plot_results(mean_train_errors, mean_test_errors, std_train_errors, std_test_errors)
    
    # è¦–è¦ºåŒ–éæ“¬åˆæ¡ˆä¾‹
    print("\nç¹ªè£½éæ“¬åˆè¦–è¦ºåŒ–ç¯„ä¾‹...")
    visualize_overfitting_example()
    
    # å»ºç«‹ç¸½çµå ±å‘Š
    create_summary_report()

if __name__ == "__main__":
    main()
