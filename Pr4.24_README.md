# Problem 4.24 - Cross Validation Analysis Summary

## 📁 檔案清單

### 程式碼
- **`Pr4.24.py`**: 主要實作程式（完整的交叉驗證實驗）

### 文檔
- **`Pr4.24_說明.md`**: 詳細理論說明與數學推導
- **`Pr4.24_圖片解說.md`**: 圖表解釋與學習現象分析

### 圖表（所有圖表為 300 DPI 高解析度）
1. **`Pr4.24_part_b.png`**: E[e₁], E[e₂], E[E_cv] 的關係圖
2. **`Pr4.24_part_c.png`**: Var[e₁] vs Var[E_cv] 比較
3. **`Pr4.24_part_e.png`**: 有效新樣本數 (N_eff) 分析
4. **`Pr4.24_part_f_individual.png`**: λ = 2.5/N 的 N_eff 分析
5. **`Pr4.24_part_f_comparison.png`**: 兩種正規化參數的比較

---

## 🎯 問題摘要

**目標**: 分析留一法交叉驗證(LOOCV)在線性迴歸中的統計特性

**實驗規模**:
- 10⁵ 次重複實驗
- 11 種不同的樣本數 (N = 18 to 118)
- 2 種正規化參數設定
- 總計訓練 2,200,000 個模型

---

## 📊 主要發現

### Part (a) - 數值結果

完整的統計量計算，包括所有 CV 誤差的平均值和變異數。

**關鍵數據** (λ = 0.05/N):

| N   | E[e₁]    | E[E_cv]  | Var[e₁]  | Var[E_cv] | N_eff/N |
|-----|----------|----------|----------|-----------|---------|
| 18  | 0.331    | 0.331    | 0.232    | 0.017     | 76.0%   |
| 68  | 0.266    | 0.266    | 0.142    | 0.002     | 93.5%   |
| 118 | 0.260    | 0.259    | 0.136    | 0.001     | 97.6%   |

### Part (b) - 平均值的關係 ✅

**理論**: E[e₁] = E[e₂] = E[E_cv]

**結果**: ✓ **完全驗證**
- 三個期望值在所有 N 下幾乎完全相同
- 差異 < 0.001（< 0.2% 相對誤差）
- 所有 CV 誤差都是無偏估計

**學習現象**:
- LOOCV 提供可靠的泛化誤差估計
- 隨著 N 增加，泛化誤差降低（0.331 → 0.259）

### Part (c) - 變異數分析 📉

**觀察**: Var[e₁] >> Var[E_cv]

**數值**:
- N = 18: Var[e₁] / Var[E_cv] = 13.67
- N = 118: Var[e₁] / Var[E_cv] = 115.22

**學習現象**:
- 單點誤差變異性大，但平均誤差非常穩定
- Var[e₁] 隨 N 遞減（0.232 → 0.136）
- 平均效應使估計更可靠

**變異數來源**:
1. 資料隨機性
2. 噪音項
3. 留出點的隨機性
4. 模型估計變異

### Part (d) - 理論分析 🔬

**獨立性假設**: 如果 e_i 完全獨立，則：
$$\text{Var}[E_{cv}] = \frac{\text{Var}[e_i]}{N}$$

**現實**: e_i 並非完全獨立
- 共享 N-2 個訓練點
- 但**驗證點完全不同**
- 導致近似獨立性

### Part (e) - 有效樣本數 🎯

**定義**: $N_{eff} = \frac{\text{Var}[e_i]}{\text{Var}[E_{cv}]}$

**結果**:
- N = 18: N_eff/N = 76%
- N = 68: N_eff/N = 94%
- N = 118: N_eff/N = **98%** ⭐

**關鍵洞察**:
✅ **N_eff ≈ N**: CV 誤差幾乎完全獨立！
- 大樣本時接近 100%
- 說明 LOOCV 充分利用數據
- 每個誤差使用不同驗證點是關鍵

### Part (f) - 正規化的影響 🔧

**假說**: 增加正規化 → N_eff 下降

**實驗**: λ = 0.05/N vs λ = 2.5/N (50倍)

**結果**:
```
N = 18:  76.0% → 74.2%  (-1.8%) ✓
N = 28:  85.4% → 84.8%  (-0.6%) ✓  
N = 68:  93.5% → 91.8%  (-1.8%) ✓
N = 118: 97.6% → 97.8%  (+0.1%) ✗
```

**結論**: 部分驗證
- ✓ 小樣本: 假說成立
- ✗ 大樣本: 效應微弱或反轉
- 驗證點獨立性主導效應

---

## 🧮 技術細節

### 演算法
```python
# Ridge Regression
w_reg = (X^T X + λI)^(-1) X^T y

# Leave-One-Out CV
for i in 1 to N:
    Train on {1,...,i-1,i+1,...,N}
    Validate on {i}
    e_i = (y_i - x_i^T w_{-i})^2

E_cv = (1/N) Σ e_i
```

### 參數設定
- d = 3 (輸入維度)
- σ = 0.5 (噪音標準差)
- λ = 0.05/N 或 2.5/N
- 實驗次數 = 10⁵

---

## 📈 圖表說明

### 1. Part (b) 圖 - 平均值關係
展示 E[e₁], E[e₂], E[E_cv] 幾乎重合，驗證理論預測。

### 2. Part (c) 圖 - 變異數比較
顯示 Var[E_cv] 遠小於 Var[e₁]，說明平均降低變異數。

### 3. Part (e) 圖 - 有效樣本數
- 左: N_eff 接近 N
- 右: N_eff/N 接近 100%

### 4. Part (f) 比較圖
展示不同正規化參數下 N_eff 的變化。

---

## 💡 關鍵學習點

### 理論驗證
1. ✅ **無偏性**: E[CV error] = E[泛化誤差]
2. ✅ **變異數降低**: 平均 N 個誤差 → Var 降低 N 倍
3. ✅ **近似獨立**: N_eff ≈ N (大樣本時 >97%)

### 實務啟示
1. **LOOCV 優點**:
   - 最大化數據利用
   - 無偏估計
   - 低變異數（透過平均）

2. **LOOCV 缺點**:
   - 計算成本高 (O(N) 次訓練)
   - 對異常值敏感

3. **正規化影響**:
   - 輕度影響 CV 誤差相關性
   - 主要作用是控制偏差-變異數權衡

### 統計洞察
- 驗證點獨立性是 LOOCV 成功的關鍵
- 即使訓練集重疊，不同的驗證點仍提供獨立信息
- 大樣本下，CV 誤差幾乎完全獨立

---

## 🚀 如何執行

### 環境需求
```bash
pip install numpy matplotlib seaborn tqdm
```

### 執行程式
```bash
python Pr4.24.py
```

### 輸出
- 控制台: 詳細數值結果和分析
- 圖片: 5 張高解析度圖表 (300 DPI)

---

## 📚 參考文獻

1. **Learning From Data** - Abu-Mostafa et al. (Problem 4.24)
2. **Stone (1977)**: Cross-validatory choice and assessment of statistical predictions
3. **Ridge Regression**: Hoerl & Kennard (1970)

---

## ✨ 總結

這個實驗完整展示了**交叉驗證的統計理論與實踐**:

- 📊 **10⁵ 次實驗**提供高精確度統計估計
- ✅ **理論完全驗證**: 無偏性、變異數關係、有效樣本數
- 🔬 **深入洞察**: CV 誤差的近似獨立性機制
- 📈 **實務指導**: 正規化、樣本數對 CV 的影響

**核心發現**: LOOCV 充分利用數據 (N_eff ≈ N)，提供可靠的泛化誤差估計！

---

*實驗完成於 2025年10月5日*  
*計算資源: M1/M2 Mac, 運行時間 ~25 分鐘*
